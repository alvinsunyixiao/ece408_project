\documentclass{article}
\usepackage{graphicx}
\graphicspath{ {./report_img/} } % graphics path
\usepackage{fancyhdr}
\usepackage{listings}
\setlength{\parindent}{0pt}
\usepackage{float}
\usepackage[letterpaper,margin=1in]{geometry}
\renewcommand{\baselinestretch}{1.2}    % line spacing
\usepackage{amsmath}
\usepackage{amssymb} % for \mathbb -- set of integers
%\usepackage[bf,large]{caption}
\usepackage{caption}
\usepackage{subcaption} % for subfigure environment
\usepackage{steinmetz}  % for \phase
\usepackage{mathrsfs}   % for \mathscr -- Laplace transform operator
\usepackage{mathtools}  % for \floor{}
\usepackage{sectsty}
\usepackage[usenames,dvipsnames]{color}
\definecolor{lightgray}{gray}{0.5}
% This is the color used for MATLAB comments below
\definecolor{MyDarkGreen}{rgb}{0.0,0.4,0.0}
\lstloadlanguages{Matlab}%
\lstset{language=bash,                        % Use MATLAB
        frame=single,                           % Single frame around code
        basicstyle=\small\ttfamily,             % Use small true type font
        %keywordstyle=[1]\color{Blue}\bf,        % MATLAB functions bold and blue
        %keywordstyle=[2]\color{Purple},         % MATLAB function arguments purple
        %keywordstyle=[3]\color{Blue}\underbar,  % User functions underlined and blue
        identifierstyle=,                       % Nothing special about identifiers
                                                % Comments small dark green courier
        commentstyle=\usefont{T1}{pcr}{m}{sl}\color{MyDarkGreen}\small,
        stringstyle=\color{Purple},             % Strings are purple
        showstringspaces=false,                 % Don't put marks in string spaces
        tabsize=2,                              % 5 spaces per tab
        %
        morecomment=[l][\color{Blue}]{...},     % Line continuation (...) like blue comment
        numbers=left,                           % Line numbers on left
        firstnumber=0,                          % Line numbers start with line 1
        numberstyle=\tiny\color{Blue},          % Line numbers are blue
        stepnumber=1,                           % Line numbers go in steps of 5
        breaklines=true
        }
\pagestyle{fancy}
%\thispagestyle{plain}

\def\class{ECE 408}
\def\fp{Final Project}

\partfont{\centering}

\lhead{ Sun, Xue, Miao, \class, \fp}

\begin{document}

\title{\fp}
\date{} % disable date to include in the title
\author{
\begin{tabular}{ccc}
    Alvin Sun & Yuqi Xue & Yan Miao \tabularnewline
    yixiaos3 & yuqixue2 & yanmiao2
\end{tabular}
}
%\maketitle

\begin{titlepage} % Suppresses displaying the page number on the title page and the subsequent page counts as page 1
	\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for horizontal lines, change thickness here

	\center % Centre everything on the page

	%------------------------------------------------
	%	Headings
	%------------------------------------------------

	\textsc{\LARGE University of Illinois at Urbana Champaign}\\[1.5cm] % Main heading such as the name of your university/college

	\textsc{\Large Applied Parallel Programming}\\[0.5cm] % Major heading such as course name

	\large Team: wandering-gpu \\[0.5cm] % Minor heading such as course title

	%------------------------------------------------
	%	Title
	%------------------------------------------------

	\HRule\\[0.4cm]

	{\huge\bfseries Final Project}\\[0.4cm] % Title of your document

	\HRule\\[1.5cm]

	%------------------------------------------------
	%	Author(s)
	%------------------------------------------------

	\begin{minipage}{0.4\textwidth}
		\begin{flushleft}
			\large
			\textit{Author}\\
			Alvin Sun \\ % Your name
            Yuqi Xue  \\
            Yan Miao
		\end{flushleft}
	\end{minipage}
	~
	\begin{minipage}{0.4\textwidth}
		\begin{flushright}
			\large
			\textit{Net ID}\\
			yixiaos3 \\ % net id
            yuqixue2 \\
            yanmiao2
		\end{flushright}
	\end{minipage}

	% If you don't want a supervisor, uncomment the two lines below and comment the code above
	%{\large\textit{Author}}\\
	%John \textsc{Smith} % Your name

	%------------------------------------------------
	%	Date
	%------------------------------------------------

	\vfill\vfill\vfill % Position the date 3/4 down the remaining page

	{\large\today} % Date, change the \today to a set date if you want to be precise

	%------------------------------------------------
	%	Logo
	%------------------------------------------------

	%\vfill\vfill
	%\includegraphics[width=0.2\textwidth]{placeholder.jpg}\\[1cm] % Include a department/university logo - this will require the graphicx package

	%----------------------------------------------------------------------------------------

	\vfill % Push the date up 1/4 of the remaining page

\end{titlepage}

% deprecated
% \part*{Credentials}
% \setcounter{section}{0}
%
% \begin{tabular}{ll}
%     \textbf{Team Name}: & wandering-gpu \tabularnewline
%     \textbf{School Affiliation}: & UIUC \tabularnewline
%     \tabularnewline
% \end{tabular} \\
% \begin{tabular}{ll}
%     \textbf{Name}: & Alvin Sun \tabularnewline
%     \textbf{NetID}: & yixiaos3 \tabularnewline
%     \textbf{RaiID}: & 5c78c49784318364bab96ed4 \tabularnewline
%     \tabularnewline
%     \textbf{Name}: & Yuqi Xue \tabularnewline
%     \textbf{NetID}: & yuqixue2 \tabularnewline
%     \textbf{RaiID}: & 5c78c49f84318364bab96ee2 \tabularnewline
%     \tabularnewline
%     \textbf{Name}: & Yan Miao \tabularnewline
%     \textbf{NetID}: & yanmiao2 \tabularnewline
%     \textbf{RaiID}: & 5c78c48d84318364bab96ec2
% \end{tabular}

\part*{Milestone 1}
\setcounter{section}{0}

\section{Kernel Statistics}
\begin{tabular}{ccccccl}
 Time(\%) & Time & Calls & Avg & Min &   Max  &  Name \tabularnewline
  40.10\% &  16.788ms &  20 & 839.42us & 1.1200us & 16.155ms & [CUDA memcpy HtoD] \tabularnewline
  20.18\% & 8.4497ms  &  1 &  8.4497ms & 8.4497ms & 8.4497ms & void cudnn::detail::implicit\_convolve\_sgemm \tabularnewline
  11.81\% & 4.9434ms  &  1 &  4.9434ms & 4.9434ms & 4.9434ms & volta\_cgemm\_64x32\_tn \tabularnewline
   7.05\% & 2.9497ms  &  2 &  1.4748ms & 25.568us & 2.9241ms & void op\_generic\_tensor\_kernel \tabularnewline
   5.69\% & 2.3830ms  &  1 &  2.3830ms & 2.3830ms & 2.3830ms & void fft2d\_c2r\_32x32 \tabularnewline
   5.59\% & 2.3404ms  &  1 &  2.3404ms & 2.3404ms & 2.3404ms & volta\_sgemm\_128x128\_tn \tabularnewline
   4.55\% & 1.9059ms  &  1 &  1.9059ms & 1.9059ms & 1.9059ms & void cudnn::detail::pooling\_fw\_4d\_kernel \tabularnewline
   4.18\% & 1.7480ms  &  1 &  1.7480ms & 1.7480ms & 1.7480ms & void fft2d\_r2c\_32x32
\end{tabular}

\section{CUDA API Statistics}
\begin{tabular}{ccccccl}
Time(\%)   &   Time   &  Calls   &    Avg   &    Min    &   Max  &  Name \tabularnewline
41.94\% & 2.94373s    &    22 & 133.81ms & 13.721us & 1.52482s & cudaStreamCreateWithFlags \tabularnewline
 34.43\% & 2.41664s    &    24 & 100.69ms & 97.853us & 2.41057s & cudaMemGetInfo \tabularnewline
 20.93\% & 1.46898s    &    19 & 77.315ms &    817ns & 393.98ms & cudaFree \tabularnewline
\end{tabular}

\section{Differences Between Kernels \& API Calls} % DONE
% Report: Include an explanation of the difference between kernels and API calls.
A CUDA kernel is an extended C function that, when called, are executed multiple times in parallel by different CUDA threads on the GPU.
The CUDA APIs are programming interfaces that allow the programmer to use the CUDA device, i.e. the GPU.
Kernel functions are written by the programmer and are meant to execute specific (mostly computation-intensive) tasks on the GPU,
while API calls are provided by the CUDA library and are to manage the CUDA runtime environment and mostly prepare for the execution of kernels.
While kernels are always executed by CUDA cores, CUDA APIs do not necessarily involve the execution of CUDA cores.

\section{MXNet CPU Execution}
\begin{lstlisting}[language=bash]
Loading fashion-mnist data... done
Loading model... done
New Inference
EvalMetric: {'accuracy': 0.8236}
\end{lstlisting}

\textbf{Run Time.} 5.06s

\pagebreak
\section{MXNet GPU Execution}
\begin{lstlisting}[language=bash]
Loading fashion-mnist data... done
Loading model... done
New Inference
EvalMetric: {'accuracy': 0.8236}
\end{lstlisting}

\textbf{Run Time:} 4.40s

\part*{Milestone 2}
\setcounter{section}{0}

% \section{CPU Execution Time}
\begin{table}[H]
    \centering
    \begin{minipage}{.32\linewidth}
        \begin{tabular}{c|c}
            Full CPU Time & 11.32s \\ \hline
            First Layer Time & 2.405296s \\ \hline
            Second Layer Time & 7.342860
        \end{tabular}
        \caption*{10000 images}
    \end{minipage}
    ~
    \begin{minipage}{.32\linewidth}
        \begin{tabular}{c|c}
            Full CPU Time & 0.250576s \\ \hline
            First Layer Time & 0.756567s \\ \hline
            Second Layer Time & 2.03s
        \end{tabular}
        \caption*{1000 images}
    \end{minipage}
    ~
    \begin{minipage}{.32\linewidth}
        \begin{tabular}{c|c}
            Full CPU Time & 1.08s \\ \hline
            First Layer Time & 0.035050s \\ \hline
            Second Layer Time & 0.075312s
        \end{tabular}
        \caption*{100 images}
    \end{minipage}
    \caption{CPU Run Time Statistics}
\end{table}

\part*{Milestone 3}
\setcounter{section}{0}

\section{Execution Summary}

\begin{table}[H]
    \centering
    \begin{minipage}{.32\linewidth}
        \begin{tabular}{c|c}
            Accuracy & 0.8397 \\ \hline
            First Layer Time & 9.252ms \\ \hline
            Second Layer Time & 19.331ms
        \end{tabular}
        \caption*{10000 images}
    \end{minipage}
    ~
    \begin{minipage}{.32\linewidth}
        \begin{tabular}{c|c}
            Accuracy & 0.852 \\ \hline
            First Layer Time & 0.677ms \\ \hline
            Second Layer Time & 1.968ms
        \end{tabular}
        \caption*{1000 images}
    \end{minipage}
    ~
    \begin{minipage}{.32\linewidth}
        \begin{tabular}{c|c}
            Accuracy & 0.84 \\ \hline
            First Layer Time & 0.121ms \\ \hline
            Second Layer Time & 0.248ms
        \end{tabular}
        \caption*{100 images}
    \end{minipage}
    \caption{GPU Run Time Statistics}
\end{table}

\section{Execution Timeline}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\linewidth}
        \includegraphics[width=\linewidth]{nvvp100an}
        \caption{Dataset Size 100}
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
        \includegraphics[width=\linewidth]{nvvp1000an}
        \caption{Dataset Size 1000}
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
        \includegraphics[width=\linewidth]{nvvp10000an}
        \caption{Datset Size 10000}
    \end{subfigure}
    \caption{Kernel Execution Timelines generated by NVVP.}
\end{figure}

\part*{Milestone 4}
\setcounter{section}{0}

\section{Execution Summary}
Upon Milestone 4, we have implemented three optimizations to our kernel function, including having different
<<<<<<< HEAD
implementations and tunings for different layers, using shared memory convolution, and putting convolution
kernels in constant memory. The overall execution involving all three optimizations yields the following
=======
implementations and tuning for different layers, using shared memory convolution, and putting convolution
kernels in constant memory. The overall execution (on server) involving all three optimizations yields the following
>>>>>>> milestone4
results.

\begin{table}[H]
    \centering
    \begin{minipage}{.32\linewidth}
        \begin{tabular}{c|c}
            Accuracy & 0.8397 \\ \hline
            First Layer Time & 3.560ms \\ \hline
            Second Layer Time & 12.878ms
        \end{tabular}
        \caption*{10000 images}
    \end{minipage}
    ~
    \begin{minipage}{.32\linewidth}
        \begin{tabular}{c|c}
            Accuracy & 0.852 \\ \hline
            First Layer Time & 0.366ms \\ \hline
            Second Layer Time & 1.180ms
        \end{tabular}
        \caption*{1000 images}
    \end{minipage}
    ~
    \begin{minipage}{.32\linewidth}
        \begin{tabular}{c|c}
            Accuracy & 0.84 \\ \hline
            First Layer Time & 0.074ms \\ \hline
            Second Layer Time & 0.149ms
        \end{tabular}
        \caption*{100 images}
    \end{minipage}
<<<<<<< HEAD
    \caption{GPU Run Time Statistics}
=======
    \caption{Run time statistics after applying all three optimizations.}
>>>>>>> milestone4
\end{table}

We will discuss each optimization in the following section.

\section{Optimizations}

<<<<<<< HEAD
\subsection{Seperate Kernel Implementations for Different Layers}
[Optimization analysis and nvprof report]

\subsection{Shared Memory Convolution}
We have already implemented different kernel functions for the two layers. Since optimizations in Section 2.1
are merely parameter tuning for each layer, we would like to directly implement further optimizations based
upon the two-kernel version in Section 2.1 and analyze the results by comparing to that version instead of
the original unoptimized version. 

\subsection{Kernels in Constant Memory}
[Optimization analysis and nvprof report]

=======
\paragraph{Note}
As a preliminary note, we would like to mention that, in order to avoid unexpected server fluctuations and speedup our development,
we have setup a local development environment using an NVIDIA GeForce RTX 2080 running CUDA 10.0. Our discussions below will be based on
execution results and statistics generated in our local environment. Despite this, we have ensured that our optimizations work as
intended on the server. Also, for simplicity, we will only benchmark data size 10000.

\subsection{Seperate Kernel Implementations for Different Layers}
As far as we observe, the original kernel function, although general enough to be used for both layers, has a function signiture
\begin{verbatim}
__global__ void forward_kernel(float *y, const float *x, const float *k,
        const int B, const int M, const int C, const int H, const int W, const int K)
\end{verbatim}
that is too long and many of the parameters are unnecessary as we already know them for each layer. Hence, one optimization involves
implementing two different kernel functions for these two different layers. In other words, this optimization is barely finding and hardcoding
the best possible parameters for each layer, including layer input and output channel numbers, input and output dimensions, and, most importantly,
kernel launch parameters that best avoid warp control divergences.
\par
By profiling our optimized version using NVIDIA Nsight Compute, we observe a great deal of increase in execution time and decrease in
warp divergences.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\linewidth}
        \includegraphics[width=\linewidth]{2kern_layer1_runtime}
        \caption{Execution Time}
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
        \includegraphics[width=\linewidth]{2kern_layer1_warp}
        \caption{Warp Statistics}
    \end{subfigure}
    \caption{Layer 1 Statistics}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\linewidth}
        \includegraphics[width=\linewidth]{2kern_layer2_runtime}
        \caption{Execution Time}
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
        \includegraphics[width=\linewidth]{2kern_layer2_warp}
        \caption{Warp Statistics}
    \end{subfigure}
    \caption{Layer 2 Statistics}
\end{figure}

\paragraph{Note}
Now, we have already implemented different kernel functions for the two layers. Since optimizations in Section 2.1
are merely parameter tuning for each layer, we would like to directly implement further optimizations based
upon the two-kernel version in Section 2.1 and analyze the results by comparing to that version instead of
the original unoptimized version.

\subsection{Kernels in Constant Memory}
One optimization is to put kernels in constant memory by using
cudaMemcpyToSymbol() API call. This optimization is possible because convolution
kernels are
effectively read-only for each CUDA kernel launch. The contribution by this
optimization alone results in a relatively small amount of improvement in runtime
due to increasing efficiency of memory accessing throughput. The constant memory
kernel alone did not boost up too much the performance due to the bottleneck
of global memory fetching induced by the input element accesses.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\linewidth}
        \includegraphics[width=\linewidth]{const_layer1_runtime}
        \caption{Execution Time}
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
        \includegraphics[width=\linewidth]{const_layer1_mem}
        \caption{Memory Throughput}
    \end{subfigure}
    \caption{Layer 1 Statistics}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\linewidth}
        \includegraphics[width=\linewidth]{const_layer2_runtime}
        \caption{Execution Time}
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
        \includegraphics[width=\linewidth]{const_layer2_mem}
        \caption{Memory Throughput}
    \end{subfigure}
    \caption{Layer 2 Statistics}
\end{figure}


\subsection{Tiled Convolution using Shared Memory}
We can also optimize memory access pattern and reduce global memory access by caching
elements that are highly reused into shared memory for each thread block.
This optimization also implies that we break the input image into smaller tiles
and each thread block processes a tile of elements at one time. \\

\par
We chose loading strategy one, where all threads are utilized in multiple loop
cycles to load the slightly bigger shared tile of data. The reason for not using
strategy 2 is that we realize the finite computing resources is one of the
biggest limiting factor of performance, so we don't want to waste the extra threads
that are active only in loading stage but idle in computing stage. From the runtime
report below, we can see that this optimization alone did not improve too much
of the overall runtime, but even
slightly hurt the performance. This is because of the bottleneck presented
in reading the kernel data from global memory. Since the kernel is quite small,
many threads are competing for the same addresses which causes a lot of serialization
in the calculation loop. The tiled strategy, however, does introduce some overhead
in the loading stage. Despite all of these, the overall memory throughput is still
increased by this optimization.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\linewidth}
        \includegraphics[width=\linewidth]{shared_layer1_runtime}
        \caption{Execution Time}
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
        \includegraphics[width=\linewidth]{shared_layer1_mem}
        \caption{Memory Throughput}
    \end{subfigure}
    \caption{Layer 1 Statistics}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\linewidth}
        \includegraphics[width=\linewidth]{shared_layer2_runtime}
        \caption{Execution Time}
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
        \includegraphics[width=\linewidth]{shared_layer2_mem}
        \caption{Memory Throughput}
    \end{subfigure}
    \caption{Layer 2 Statistics}
\end{figure}

\subsection{Combined Optimization}
Now we combine all the optimizations to benchmark against our Two Kernel baseline
implementation, we can see a great deal of improvement in runtime, memory throughput,
and SM occupancy. From the statistics below we can see that a lot less threads
are stalling for memory operations and more time are spent in floating point
computation. The combination of constant cached kernel and tiled convolution
completely takes away global memory access in the computing stage. And since shared
tile loading is almost perfectly coealesed with this linearized strategy, the
runtime is dramatically reduced.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\linewidth}
        \includegraphics[width=\linewidth]{ms4_layer1_runtime}
        \caption{Execution Time}
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
        \includegraphics[width=\linewidth]{ms4_layer1_warp}
        \caption{Warp Statistics}
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
        \includegraphics[width=\linewidth]{ms4_layer1_mem}
        \caption{Memory Throughput}
    \end{subfigure}
    \caption{Layer 1 Statistics}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\linewidth}
        \includegraphics[width=\linewidth]{ms4_layer2_runtime}
        \caption{Execution Time}
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
        \includegraphics[width=\linewidth]{ms4_layer2_warp}
        \caption{Warp Statistics}
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
        \includegraphics[width=\linewidth]{ms4_layer2_mem}
        \caption{Memory Throughput}
    \end{subfigure}
    \caption{Layer 2 Statistics}
\end{figure}
>>>>>>> milestone4

\end{document}
